#!/usr/bin/python
"""
Unified Test Runner
"""

import os
import sys
import platform
import argparse
import logging
import time
import getpass
import glob
import types
import inspect
import subprocess
import traceback
import csv
import _csv

if sys.version_info[0] == 3:
    from io import StringIO

    basestring = str
elif sys.version_info[0] == 2:
    import StringIO
import re
import yaml
from io import open

from collections import namedtuple
from datetime import datetime
from junit_xml import TestSuite, TestCase
from pkg_resources import parse_version
from qacomplete import QACConnection, QACConnectionError

from unified_modules.test_support.testbaseclass import BaseTest
from unified_modules.test_support.testresults import PASS, FAIL, ABORT, SKIPPED, BLOCKED, RETURN_CODES
from unified_modules.test_support.testutils import get_stack_trace
from unified_modules.results.result_record import ResultRecord

VALID_TEST_RESULT = [PASS, FAIL, ABORT, SKIPPED, BLOCKED]
INVALID_TESTCASE_STATUS = ['Suspended', 'Obsolete']

TEST_STATUS_MAPPING = {PASS: 'Passed',
                       FAIL: 'Failed',
                       ABORT: 'Failed',
                       SKIPPED: 'Skipped',
                       BLOCKED: "Blocked"}

QAC = 'QAC'
NON_QAC = 'NON_QAC'


class BaseTestRunnerException(Exception):
    """
    Exception class for BaseTestRunner
    """
    pass


class CampaignTest(object):
    """
    A simple data structure to store test case details that is executed as part of a test campaign.

    CampaignTest objects are created at filter stage and then enriched during
    the test execution process. Their only purpose is to carry information for a
    particular test exposed as public attributes.
    """

    def __init__(self, name=None, ts_details=None, test_id=None, test_inputs=None,
                 target_device=None, test_platform=None, log_messages=None, results_folder_flag=False,
                 is_blocked=False):
        self.name = name
        self.ts_details = ts_details
        self.identifier = test_id
        if test_inputs:
            self.test_inputs = test_inputs
        else:
            self.test_inputs = {}
        self.return_code = None
        self.outcome = SKIPPED
        self.result_file = None
        self.record = None
        self.log_file = None
        self.junit_result = None
        self.run_time = None
        self.target_device = target_device
        self.test_platform = test_platform
        self.log_messages = log_messages
        self.results_folder_flag = results_folder_flag
        self.is_blocked = is_blocked

    def __str__(self):
        """String representation of object."""
        return "%s" % self.name


class TestScriptDetails(object):
    """
    A simple data structure to store test script details on the filesystem.

    TestScriptDetails objects are created at discovery stage and then used for constructing CampaignTest objects during
    the test execution process. Their only purpose is to carry information related to a test script
    that could be executed for 1 or more test cases with different test inputs.
    """

    def __init__(self, name=None, description=None, source_file=None,
                 devices=None, campaigns=None, constructor=None, runable=True, error_msg=None):
        self.name = name
        self.description = description
        self.source_file = source_file
        self.devices = devices
        self.campaigns = campaigns
        self.constructor = constructor
        self.runable = runable
        self.error_msg = error_msg

    def __str__(self):
        """String representation of object."""
        return "%s" % self.name


class BaseTestRunner(object):
    """
    The base class for any test runner. Defines a five step test execution
    process:

    - The ``setup()`` step deals with runner start-up and initialisation but
      also run time options.

    - The ``discover()`` step analyses the file system looking for any tests it
      can find. By default any ``*.py`` matching file will be analysed.

    - The ``filter()`` step narrows down the set of discovered tests according
      to run time options. By default tests are filtered by target device and
      sorted alphabetically.

    - The ``execute()`` step runs test. By default tests are run in
      sub-processes.

    - The ``report()`` step produces various output based on the result of the
      execution phase. By default, a result matrix will be logged and a JUnit
      file will be generated.
    """

    def __init__(self, runner_type=NON_QAC):
        """
        Construct an empty :obj:`BaseTestRunner` base runner instance.
        """
        self.exec_type = None
        self.project_type = None
        self.um_style = True
        self.target_device = None
        self.test_platform = None
        self.run_in_process = False
        self.individual_results = True
        self.dry_run = False
        self.skip_folders = ''
        self.reporters = list()
        self.result_folder = None
        self.log = None
        self.test_names = list()
        self.options = None
        self.results_folder_flag = False
        self.runner_type = runner_type
        file_ext = namedtuple('TestsExt', 'py rb adb_UIAuto')
        self.file_type = file_ext('py', 'rb', 'adb_UIAuto')
        project_type_list = namedtuple('ProjectType', 'firmware tools')
        self.project_type_list = project_type_list('firmware', 'tools')
        self.debug = None
        self.testconfiguration = dict()
        self.rerun_test = False
        self.rerun_abort_test = False
        self.is_sequential = False
        self.rerun_testset = False
        self.testrun_id = None
        self.tests_pattern = None

    @staticmethod
    def _detect_platform():
        """
        Finds out which system the code runs on.

        Returns:
            str: a simple string representing the platform.

        Raises:
            OSError: if detected platform is unsupported.
        """
        if platform.system() == 'Windows':
            if platform.release() in ['10', '8']:
                return 'win10'
            elif platform.release() == '7':
                return 'win7'

        if platform.system() == 'Darwin':
            return 'macOS'

        raise OSError("Platform not supported")

    def _setup_logging(self, log_prefix, log_folder, log_timestamp=None):
        """
        Prepare the logger instance to be used to trace runner events.

        Args:
            log_prefix (:obj:`str`): the prefix string to be use for logging.
            log_folder (:obj:`str`): the destination folder for the log file.
            log_timestamp (:obj:`str`, optional): a timestamp used in the log
                filename. Defaults to `None` which means current time.

        Raises:
            OSError: if given log destination folder does not exist and could
                not be created.
        """
        log = logging.getLogger(log_prefix)

        log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')

        log_stream_handler = logging.StreamHandler()
        log_stream_handler.setFormatter(log_formatter)

        log.addHandler(log_stream_handler)

        try:
            if not os.path.exists(log_folder):
                os.mkdir(log_folder)
        except OSError as error:
            print(str(error))
            return log

        if not log_timestamp:
            log_timestamp = time.strftime('%Y%m%d%H%M%S')
        log_filename = '%s-%s.log' % (log_prefix, log_timestamp)
        log_filename = os.path.join(log_folder, log_filename)

        log_file_handler = logging.FileHandler(log_filename)
        log_file_handler.setFormatter(log_formatter)

        log.addHandler(log_file_handler)

        return log

    @staticmethod
    def get_argument_parser():
        """
        Return a new argument parser with base runner parameters.

        Note:
            Runner implementations should always create command line parsers
            using it and add their specific options on top of it.

        Returns:
            argparse.ArgumentParser: a newly created base argument parser.
        """
        parser = argparse.ArgumentParser(description='Unified Modules test runner.')

        # Common runner input arguments
        parser.add_argument('--config',
                            help='Runner configuration yml file path',
                            type=str, default=None, required=False)
        parser.add_argument('--non_tms',
                            help='No TMS interaction',
                            action='store_true', required=False)
        parser.add_argument('--non_um',
                            help='Non-UM style test framework.'
                                 'Test script class not inherited from UM Core BaseTest class '
                                 'or any of the UMF classes',
                            action='store_true', required=False)
        parser.add_argument('--tests_path',
                            help='comma-separated list of test project paths.',
                            type=str, default=None, required=False)
        parser.add_argument('--exec_type',
                            help='File executor type. Supported:py,rb,adb_UIAuto',
                            type=str, default="py", required=False)
        parser.add_argument('--project_type',
                            help='Project type for Non UM style test framework. Supported:firmware, tools',
                            type=str, default=None, required=False)
        parser.add_argument('--debug',
                            help='Enable debug mode for non-UM style firmware and tools test execution.',
                            action='store_true', required=False)
        parser.add_argument('--target_device',
                            help='targeted device under test (DUT) name.',
                            type=str, default=None, required=False)
        parser.add_argument('--result_folder',
                            help='relative or absolution path to result folder.',
                            type=str, default=None, required=False)
        parser.add_argument('--test_platform',
                            help='test platform name (override automatic detection).',
                            type=str, default=None, required=False)
        parser.add_argument('--in_process',
                            help='run tests in-process (no sub-processes).',
                            action='store_true', required=False)
        parser.add_argument('--dry_run',
                            help='simulate campaign execution.',
                            action='store_true', required=False)
        parser.add_argument('--verbose',
                            help='increase log verbosity.',
                            action='count', default=0, required=False)
        parser.add_argument('--skip_folders',
                            help='comma-separated list of folders to exclude searching for tests',
                            type=str, default=".git, input_data", required=False)
        parser.add_argument('--results_folder_flag',
                            help='Creates separate folder for every test case and place all test case related files '
                                 'in it',
                            action='store_true', required=False)

        # Applicable to non-tms (standalone runner)/tms runner
        parser.add_argument('--campaign',
                            help='non_tms runner input: comma-separated list of tests, modules or campaigns to run.'
                                 'tms runner input: comma-separated list of CirrusTestIds to run.',
                            type=str, default='all', required=False)

        parser.add_argument('--tc_keys',
                            help='non_tms runner input: comma-separated list of tc_keys.',
                            type=str, default=None, required=False)

        parser.add_argument('--exclude',
                            help='non_tms runner input: comma-separated list of tests, modules or campaigns to '
                                 'exclude.'
                                 'tms runner input: comma-separted list of CirrusTestIds to exclude',
                            type=str, default=None, required=False)
        parser.add_argument('--intersect',
                            help='non_tms runner input: comma-separated list of tests, modules or campaigns to '
                                 'intersect.',
                            type=str, default=None, required=False)

        # Applicable to QAC runner
        parser.add_argument('--username',
                            help='QAC runner input: QAC username',
                            type=str, default=None, required=False)
        parser.add_argument('--password',
                            help='QAC runner input: QAC password, this can be supplied from stdIn or env var QacPassword',
                            type=str, default=None, required=False)
        parser.add_argument('--QacHost',
                            help='QAC runner input: The QAC servers hostname',
                            type=str, default=None, required=False)
        parser.add_argument('--projectname',
                            help='QAC runner input: QAC project name',
                            type=str, default=None, required=False)
        parser.add_argument('--releasename',
                            help='QAC runner input: QAC release name',
                            type=str, default=None)
        parser.add_argument('--buildId',
                            help='QAC runner input: Jenkins build id',
                            type=str, default=None)
        group = parser.add_mutually_exclusive_group(required=False)
        group.add_argument('--testsetname',
                           help="QAC testsetname name. Accepts a list of test set names separated by ';'",
                           type=str, default=None)
        group.add_argument('--testsetfoldername',
                           help='QAC runner input: QAC folder name to find test sets',
                           type=str, default=None),
        parser.add_argument('--test_config',
                            help='Test configuration name',
                            type=str, default=None, required=False)
        parser.add_argument('--rerun_test',
                            help='To rerun failed and aborted tests',
                            action='store_true', required=False)
        parser.add_argument('--rerun_abort_test',
                            help='To rerun only aborted tests',
                            action='store_true', required=False)
        parser.add_argument('--rerun_testset',
                            help='Run failed tests by rerunning testset',
                            action='store_true', required=False)
        parser.add_argument('--testrun_id',
                            help='Specific testrun id to rerun failed tests.',
                            type=int, default=None, required=False)
        parser.add_argument('--tests_pattern',
                            help='Specific pattern of Test Scripts to run tests.',
                            type=str, default=None, required=False)

        return parser

    def setup(self):
        """
        Base runner ``setup()`` implementation: sets-up the logger, exposed as
        the `log` attribute, and deals with base options.

        Note:
            Sub-classes should always chain-up to that base implementation
            before accessing any public base attributes (especially ``log``),
            if they need to override that method.
        Returns:
            list of str: the list of paths to be analysed.
        """
        if not self.options.result_folder:
            login_name = getpass.getuser()
            default_result_folder = login_name + '_DEFAULT_RESULTS_LABEL'
            self.result_folder = os.environ.get('RESULTS_LABEL_NAME', default_result_folder)
        else:
            if not os.path.isabs(self.options.result_folder):
                self.result_folder = os.path.join(os.getcwd(), self.options.result_folder)
            else:
                self.result_folder = self.options.result_folder
        # Force RESULTS_LABEL_NAME value to the determined folder:
        os.environ['RESULTS_LABEL_NAME'] = self.result_folder

        self.log = self._setup_logging(self.__class__.__name__, log_folder=self.result_folder)

        if self.options.verbose > 0:
            self.log.setLevel(logging.DEBUG)
        else:
            self.log.setLevel(logging.INFO)

        self.log.info("Test-runner starting up...")

        self.exec_type = self.options.exec_type
        if self.options.project_type in list(self.project_type_list):
            self.project_type = self.options.project_type
        self.debug = self.options.debug
        if self.options.non_um or self.options.project_type:
            self.um_style = False
        if not self.options.test_platform:
            self.test_platform = self._detect_platform()
        else:
            self.test_platform = self.options.test_platform
        self.target_device = self.options.target_device
        self.run_in_process = self.options.in_process
        self.dry_run = self.options.dry_run
        self.results_folder_flag = self.options.results_folder_flag
        self.skip_folders = self.options.skip_folders.split(", ")

        self.log.info("Result folder set to '%s'.", self.result_folder)
        self.log.info("Test scripts file execution type set to '%s'.", self.exec_type)
        self.log.info("UM Style Test Framework:'%s'.", self.um_style)
        self.log.info("Test platform set to '%s'.", self.test_platform)
        self.log.info("Target device set to '%s'.", self.target_device)

        # Reporters can be any kind of callable object (bounded method here)
        # that accepts, as first argument, a list of test object to operate on.
        # Additional arguments can be passed, declared here as a dictionnary
        # that will get unpacked at execution time. Reporters are stored as a
        # tuple containing, in respective order: the callable object and the
        # additional argument dictionary object.

        self.reporters += [(
            self._log_result_matrix, {
                'logger': self.log,
            }
        )]

        if not self.dry_run:
            self.reporters += [(
                self._generate_junit_file, {
                    'logger': self.log,
                    'result_folder': self.result_folder,
                    'generate_individual_results': self.individual_results,
                }
            )]
        return self.options.tests_path.split(',')

    @staticmethod
    def _get_desc_form_doc(docstring):
        """
        Builds a one-line description string from a given docstring.

        Args:
            docstring (:obj:`str`): the docstring to parse.

        Returns:
            str: the extracted description, `'None'` on failure.
        """
        if not docstring:
            return 'None'
        docstring = docstring.strip()

        description = str()
        for line in docstring.split('\n'):
            line = line.strip()
            if line.startswith('@'):
                continue
            if not line:
                break
            description += line + ' '

        return description.strip()

    def find_test_in_module(self, test_module, test_module_name):
        """
        Inspects a Python module, looking for a test class.

        Args:
            test_module (:obj:`module`): the input Python module.
            test_module_name (:obj:`str`): the corresponding module's name.

        Returns:
            class: the class object for the test, :obj:`None` if not found.
        """
        # inspect.getmodule() looks for modules in sys.modules:
        sys.modules[test_module_name] = test_module

        test_class = None
        for _, class_object in inspect.getmembers(test_module):
            # We are only interested in test classes:
            if not inspect.isclass(class_object):
                continue

            class_module = inspect.getmodule(class_object)
            # Locally declared BaseTest subclass are considered test classes:
            if class_module is not test_module:
                continue
            if not issubclass(class_object, BaseTest):
                continue

            test_class = class_object
            break

        del sys.modules[test_module_name]
        return test_class

    def load_test_from_file(self, test_file):
        """
        Opens and loads code from a file, looking for a test.

        Args:
            test_file (:obj:`str`): the full path to the Python module to load.

        Returns:
            tuple: the newly created Python module and the found test class
                object, ``(None, None)`` on failure.
        """
        test_description = 'None'
        test_class = None
        test_content = None
        if not os.path.exists(test_file):
            return None, None, test_description
        test_module_name = os.path.basename(test_file)
        test_module_name = os.path.splitext(test_module_name)[0]

        if self.um_style:
            # UM Style Test Framework only support Python execution type.
            test_module = types.ModuleType(test_module_name)
            with open(test_file, encoding='utf-8') as test_content:
                try:
                    testcontent = test_content.read()
                    exec(testcontent, test_module.__dict__)
                except Exception as error:
                    self.log.debug("Import failed for '%s': %s.", test_module_name, str(error))
                    self.log.debug(traceback.format_exc())
                    err_msg = traceback.format_exc()
                    if test_content:
                        test_class = re.search(r'class\s([A-Za-z0-9]+)\(', testcontent)
                    if test_class:
                        test_class = test_class.group(1)
                    test_description = self._get_desc_form_doc(test_module.__doc__)

                    return None, test_class, test_description, err_msg

            test_class = self.find_test_in_module(test_module, test_module_name)

            if test_module.__doc__:
                test_description = self._get_desc_form_doc(test_module.__doc__)
            elif test_class.__doc__:
                test_description = self._get_desc_form_doc(test_class.__doc__)
        else:
            # Non-UM style frameworks: Fruitsalad(ruby scripts), adb UIautomator(java scripts)
            # Search for Java/Ruby style class names and test description
            test_module = test_module_name
            find_classname = r'class (.*)|.* class (\S*)\s+.*'
            find_description = r'#+\s+@brief\s+(.*)|\s*@brief\s+(.*)'
            with open(test_file, encoding='utf-8') as test_content:
                file_content = test_content.read()
                classnames = re.findall(find_classname, file_content)
                # this ruby/java script contains more than a single test class. This cannot be the test class
                # as per the default way of defining a fruitsalad/vega android test script.
                if len(classnames) != 1:
                    return None, None, test_description, None
                else:
                    test_class = next(item for item in classnames[0] if item is not None)
                    descr_match = re.findall(find_description, file_content)
                    if descr_match:
                        test_description = next(item for item in descr_match[0] if item is not None)

        return test_module, test_class, test_description, None

    def _list_tests_in_folder(self, test_path, globbing_pattern):
        """
        Builds a set of test script details for a given folder(recursive analysis)

        Args:
            test_path (:obj:`str`): the full path to the folder to analyse.
            globbing_pattern (:obj:`str`): the globbing pattern to use in order
                to select candidate files.

        Returns:
            set of :obj:`TestScriptDetails`: the discovered set of test scripts details.
        """
        matching_files = glob.glob(os.path.join(test_path, globbing_pattern))
        sub_test_dirs = os.walk(test_path)
        sub_test_dirs = [sub_test_dir[0] for sub_test_dir in sub_test_dirs]
        sub_test_dirs.remove(test_path)
        for sub_test_dir in sub_test_dirs:
            # skip adding tests if test is in any of skip_folders
            if not any(folder_name in sub_test_dir for folder_name in self.skip_folders):
                matching_files.extend(glob.glob(os.path.join(sub_test_dir, globbing_pattern)))
        test_scripts_set = set()
        for matching_file in matching_files:
            if not os.path.exists(matching_file):
                continue

            self.log.debug("Analysing '%s'.", matching_file)

            test_module, test_class, test_description, err_msg = self.load_test_from_file(matching_file)

            if test_class is None:
                continue
            if (self.um_style) and not isinstance(test_class, basestring):
                class_name = test_class.__name__
            else:
                class_name = test_class
            self.log.debug("Found test '%s' in '%s'.", class_name, matching_file)
            test_name = class_name
            if not class_name:
                test_name = test_module

            if hasattr(test_class, 'applicability_tags'):
                devices_applicability = frozenset(getattr(test_class,
                                                          'applicability_tags'))
            else:
                devices_applicability = frozenset()

            if hasattr(test_class, 'campaigns'):
                campaigns_applicability = frozenset(getattr(test_class,
                                                            'campaigns'))
            else:
                campaigns_applicability = frozenset()

            test = TestScriptDetails(
                name=test_name,
                description=test_description,
                source_file=matching_file,
                devices=devices_applicability,
                campaigns=campaigns_applicability,
                constructor=test_class,
                error_msg=err_msg
            )

            test_scripts_set.add(test)

        return test_scripts_set

    def discover(self, test_paths):
        """
        Base runner `discover()` implementation: looks for test in a given list
        of test project folders. Returns a `set` of discovered tests represented
        as `CampaignTest` objects.

        Note:
            Sub-classes, most probably, won't need to override that method.

        Args:
            test_paths (:obj:`list`): the list of test projects path to analyse.

        Returns:
            set of :obj:`CampaignTest`: the set of discovered tests.
        """
        globbing_pattern = '*.' + self.exec_type
        if self.exec_type == self.file_type.adb_UIAuto:
            globbing_pattern = '*.java'
        if self.project_type:
            raise BaseTestRunnerException("Something went wrong because we are in discover stage for non-UM style "
                                          "HST test framework. Must skip discover stage for HST type of non-UM style "
                                          "test frameworks.")

        self.log.debug("Full globbing pattern is '%s'.", globbing_pattern)
        test_scripts_set = set()
        working_directory = os.getcwd()
        for test_path in test_paths:
            if not os.path.isabs(test_path):
                test_path = os.path.abspath(test_path)
            if not os.path.exists(test_path):
                self.log.warning("Given test path '%s' does not exist...", test_path)
                continue

            self.log.info("Looking for tests in '%s'.", test_path)
            # Most likely, tests modules will perform relative imports:
            sys.path.insert(0, test_path)
            os.chdir(test_path)

            test_scripts_set.update(self._list_tests_in_folder(test_path, globbing_pattern))

            # Always restore the working directory:
            os.chdir(working_directory)

        if len(test_scripts_set):
            self.log.info("%s tests discovered in the tests_path folder(s)", len(test_scripts_set))
            test_names = ",".join(["%s" % test_script for test_script in test_scripts_set])
            self.log.info("List of test scripts: %s", test_names)
        else:
            raise BaseTestRunnerException("0 tests discovered in tests_path %s."
                                          " Problem could be either you didn't set the PROJECT_MAPPING_CONFIG env "
                                          "correctly OR you are not installed UMF with PIP along with UM "
                                          "Core and Resources " %
                                          test_paths)

        return test_scripts_set

    def filter(self, test_scripts_set):
        """
        Base runner `filter()` implementation: filters a given set of tests
        based on their targeted device property.

        Note:
            Sub-classes should always chain-up to that base implementation in
            order to respect base filtering behaviour, if they need to override
            that method.

        Args:
            test_scripts_set (:obj:`set`): the set of ``TestScriptDetails`` test scripts objects to filter.

        Returns:
            list of :obj:`CampaignTest`: an alphabetically sorted list of tests.
        """
        len_discovered = len(test_scripts_set)

        # Filter on target device and sort tests by name:
        if self.target_device:
            if self.runner_type == NON_QAC:
                test_scripts_set = [t for t in test_scripts_set
                                    if t.ts_details.devices and self.target_device in t.ts_details.devices]
            elif self.runner_type == QAC:
                tests_target_device = []
                for test in test_scripts_set:
                    if test.ts_details.devices and self.target_device in test.ts_details.devices:
                        test.ts_details.runable = True
                    else:
                        test.ts_details.runable = False
                        test.return_code = RETURN_CODES[SKIPPED]
                        self.log.warning("Test %s Skipped due to target_device input:%s mismatch "
                                         "with test script target device:%s",
                                         test.name, self.target_device, test.ts_details.devices)
                    tests_target_device.append(test)
                test_scripts_set = tests_target_device

            if len(test_scripts_set):
                test_names = ",".join(["%s" % test_script for test_script in test_scripts_set])
                self.log.info("List of test scripts applicable to %s target device: %s", self.target_device, test_names)
            else:
                warning_msgs = "\n".join(["0 tests out of %s discovered test scripts applicable to "
                                          "target device %s. Check for typos in target_device, tests_path "
                                          "inputs." % (len_discovered, self.target_device)])
                raise BaseTestRunnerException(warning_msgs)

        print("Are Tests are running Sequential?: {}".format(self.is_sequential))
        if self.runner_type == QAC and self.is_sequential:
            filtered_set = test_scripts_set
        else:
            filtered_set = sorted(test_scripts_set, key=lambda test: test.name)

        if len(filtered_set):
            self.log.info("%s tests left after filtering.", len(filtered_set))
            test_names = ",".join(["%s" % test_script for test_script in filtered_set])
            self.log.info("Final list of test scripts filtered for execution: %s", test_names)
        else:
            warning_msgs = "\n".join(["0 tests left after filtering.",
                                      "Check if your tests_path input is pointing to the "
                                      "right folder containing test scripts.",
                                      "Check if your QAC test case has the Cirrus_test_id set"])
            raise BaseTestRunnerException(warning_msgs)
        return filtered_set

    def _load_result_file(self, test_name, start_time):
        """
        Searches for and loads result file for latest ran test.

        Args:
            test_name (:obj:`str`): the name of the test to look results for.
            start_time (:obj:`float`): a timestamp for the beginning of the test
            execution.

        Returns:
            tuple: the path of the found file and the corresponding
                ``ResultRecord`` object, ``(None, None)`` if not found.
        """
        globbing_pattern = '*%s*.csv' % test_name
        if self.results_folder_flag:
            folder_name = "-".join([test_name, self.target_device, self.test_platform])
            result_files = glob.glob(os.path.join(self.result_folder, folder_name, globbing_pattern))
        else:
            result_files = glob.glob(os.path.join(self.result_folder, globbing_pattern))
        if not result_files:
            return None, None

        latest_result_file = max(result_files, key=os.path.getctime)
        if not os.path.getctime(latest_result_file) > start_time:
            return None, None

        test_name = None
        chip_name = None
        test_platform = None
        log_file = None
        result_metrics = None
        result_record = ResultRecord()
        try:
            if sys.version_info[0] == 3:
                lines = []
                # Parse the file into lines
                with open(latest_result_file, 'r') as f:
                    for line in f:
                        if not line.isspace():
                            lines.append(line)
                # Write them back to the file
                with open(latest_result_file, 'w') as f:
                    f.writelines(lines)
            with open(latest_result_file, encoding='utf-8') as result_file:
                # Set the csv field limit to max for stress tests.
                if sys.version_info[0] == 3:
                    csv.field_size_limit(min(sys.maxsize, 2147483646))
                elif sys.version_info[0] == 2:
                    csv.field_size_limit(sys.maxint)
                csv_reader = csv.reader(result_file)
                for record_name, record_value in csv_reader:
                    if record_name == 'TEST ID':
                        result_record.set_test_id(record_value)
                    elif record_name == 'TEST NAME':
                        test_name = record_value
                        result_record.set_test_name(record_value)
                    elif record_name == 'CHIP NAME':
                        chip_name = record_value
                    elif record_name == 'TEST_PLATFORM:':
                        test_platform = record_value
                    elif record_name == 'RESULTS_LABEL:':
                        result_record.set_results_label(record_value)
                    elif record_name == 'RESULT':
                        result_record.set_result(record_value)
                    elif record_name == 'START TIME':
                        result_record.set_start_time(record_value)
                    elif record_name == 'STOP TIME':
                        result_record.set_stop_time(record_value)
                    elif record_name == 'RECORD TIME':
                        result_record.set_record_time(record_value)
                    elif record_name == 'Logfile':
                        log_file = record_value
                    elif record_name == 'RESULTS METRICS':
                        result_metrics = record_value
        except (IOError, _csv.Error, ValueError, OverflowError):
            self.log.warning("Unable to generate the csv test report file "
                             "for test %s, stacktrace:%s" % (test_name, get_stack_trace()))
            return None, None

        result_record.set_basic_details(test_name, chip_name, test_platform)

        if log_file and ' - ' in log_file:
            log_file = log_file.split(' - ')[:-1]
            log_file = ' - '.join(log_file)
            result_record.set_logfile(log_file)

        if result_metrics and ' - ' in result_metrics:
            result_metrics = result_metrics.split(' - ')[:-1]
            result_metrics = ' - '.join(result_metrics)
            if result_metrics.startswith('"'):
                result_metrics = result_metrics[1:]
            result_metrics_list = result_metrics.split('\n')
            result_record.set_messages(result_metrics_list)
        return latest_result_file, result_record

    def _run_test_out_of_process(self, test, extra_arguments=None):
        """
        Executes a given test in a sub-process and block as long as the
        sub-process is running.

        Args:
            test (:obj:`CampaignTest`): the test to execute.
            extra_arguments (:obj:`list`, optional): a list of extra command
                line arguments to pass to the sub-process at execution time.

        Returns:
            int: an integer code indicating either success (0), aborting (-1) or
                failure (1).
        """
        if self.um_style:
            script_exe = sys.executable
            command_line = [
                script_exe,
                test.ts_details.source_file,
            ]
            if test.test_platform:
                command_line += ['--test_platform', test.test_platform]
            elif self.test_platform:
                command_line += ['--test_platform', self.test_platform]

            if test.target_device:
                command_line += ['--target_device', test.target_device]
            elif self.target_device:
                command_line += ['--target_device', self.target_device]

            if test.results_folder_flag:
                command_line += ['--results_folder_flag']
            elif self.results_folder_flag:
                command_line += ['--results_folder_flag']

            if extra_arguments:
                command_line += extra_arguments
            if test.test_inputs:
                for test_input in test.test_inputs.keys():
                    command_line += ["--%s" % test_input, str(test.test_inputs[test_input])]
        else:
            timestamp = time.strftime('%Y%m%d%H%M%S')
            test_log_file = "%s-%s.log" % (test.name, timestamp)
            test_log_file = os.path.join(self.result_folder, test_log_file)
            if self.exec_type == self.file_type.rb:
                # TDB how to get the system path of ruby
                script_exe = 'ruby.exe'
                command_line = [
                    script_exe,
                    test.ts_details.source_file,
                ]
                if test.test_inputs:
                    for test_input in test.test_inputs.keys():
                        command_line += ["-%s" % test_input, str(test.test_inputs[test_input])]
            elif self.exec_type == self.file_type.adb_UIAuto:
                test_module_name = os.path.basename(test.ts_details.source_file)
                test_module_name = os.path.splitext(test_module_name)[0]
                command_line = ['adb', 'shell', 'uiautomator', 'runtest', 'UIAutomator_tests.jar']
                if test.test_inputs:
                    for test_input in test.test_inputs.keys():
                        if test_input == 'c':
                            embed_test_script = str(test.test_inputs[test_input]) + ".%s" % test_module_name
                            command_line += ["-%s" % test_input, embed_test_script]
                else:
                    command_line += ['-c', "com.android.bsa.tests.%s" % test_module_name]
            elif self.project_type:
                script_exe = sys.executable
                command_line = [
                    script_exe,
                    "run.py",
                    test.ts_details.name,
                ]
                if self.debug:
                    command_line += ['-d']

        start_time = time.time()
        self.log.debug("Running command line '%s'", command_line)

        try:
            if self.um_style:
                sub_process = subprocess.Popen(command_line,
                                               stderr=subprocess.PIPE,
                                               universal_newlines=True,
                                               bufsize=1)
                error_logs = None
                if sub_process.stderr:
                    for line in iter(sub_process.stderr.readline, ''):
                        self.log.info(str(line.rstrip()))
                        if error_logs is not None:
                            error_logs += line
                        else:
                            error_logs = line
                        sub_process.stderr.flush()
                test.log_messages = error_logs
                return_code = sub_process.wait()
            else:
                sub_process = subprocess.Popen(command_line,
                                               stdout=subprocess.PIPE,
                                               stderr=subprocess.STDOUT,
                                               universal_newlines=True)
                with file(test_log_file, 'w') as log_file:
                    for line in iter(sub_process.stdout.readline, ''):
                        sys.stdout.write(line)
                        log_file.write(line)
                    return_code = sub_process.wait()
        except (IOError, OSError, WindowsError) as error:
            self.log.warning("Execution failed for '%s': %s.", test.name, str(error))
            self.log.debug(traceback.format_exc())

            test.return_code = RETURN_CODES[ABORT]
            test.outcome = ABORT
            return RETURN_CODES[ABORT]

        if self.um_style:
            result_file, result_record = self._load_result_file(test.ts_details.name, start_time)
            if result_record is None:
                self.log.warning("Execution produced no result for '%s'.", test.name)
                # Test might have aborted in __init__
                test.return_code = RETURN_CODES[ABORT]
                test.outcome = ABORT
                return RETURN_CODES[ABORT]

            result_outcome = result_record.get_result()
            if return_code == 0 and result_outcome != PASS:
                if result_outcome == FAIL:
                    return_code = RETURN_CODES[FAIL]
                elif result_outcome == ABORT:
                    return_code = RETURN_CODES[ABORT]

            # Test id may already be set, preserve it if so:
            if not test.identifier:
                test.identifier = result_record.get_test_id()
            test.return_code = return_code
            test.outcome = result_outcome
            test.result_file = result_file
            test.record = result_record
            test.log_file = result_record.get_logfile()
            test.name = result_record.test_name
            test.run_time = result_record.get_run_time()
        else:
            test.run_time = time.time() - start_time
            test.log_file = test_log_file
            if self.exec_type == self.file_type.adb_UIAuto:
                try:
                    # TBD: Priya will make UIAutomator test scripts return a return code
                    with open(test_log_file) as java_logs:
                        if 'OK (1 test)' in java_logs.read():
                            return_code = RETURN_CODES[PASS]
                        else:
                            return_code = RETURN_CODES[FAIL]
                except IOError:
                    self.log.error("No such file or directory:%s", test_log_file)
                    return_code = RETURN_CODES[FAIL]
            if self.project_type:
                try:
                    with open(test_log_file) as test_log:
                        logs = test_log.read()
                        log_msg = " ".join(logs.split()).encode('utf-8')
                        if TEST_STATUS_MAPPING[PASS] in log_msg:
                            test.log_messages = "PASSED"
                            return_code = RETURN_CODES[PASS]
                        elif TEST_STATUS_MAPPING[FAIL] in log_msg:
                            match_con = re.search('Stack Trace:(.*)', log_msg)
                            test.log_messages = match_con.group(1)
                            return_code = RETURN_CODES[FAIL]
                        elif TEST_STATUS_MAPPING[SKIPPED] in log_msg:
                            test.log_messages = log_msg
                            return_code = RETURN_CODES[SKIPPED]
                        else:
                            test.log_messages = "ABORTED"
                            return_code = RETURN_CODES[ABORT]
                except IOError:
                    self.log.error("No such file or directory:%s", test_log_file)
                    return_code = RETURN_CODES[FAIL]

            test.return_code = return_code
            test.outcome = RETURN_CODES.keys()[RETURN_CODES.values().index(return_code)]
        return return_code

    def _run_test_in_process(self, test, extra_arguments):
        """
        Executes a given test in-process.

        Args:
            test (:obj:`CampaignTest`): the test to execute.
            extra_arguments (:obj:`list`, optional): a list of extra command
                line arguments to pass to the sub-process at execution time.

        Returns:
            int: an integer code indicating either success (0), aborting (-1) or
                failure (1).
        """

        # TODO: Investigate in-process test execution...

        return -1

    def execute(self, test_list, extra_arguments):
        """
        Base runner ``execute()`` implementation: runs a list of tests in the
        order they've been given.
        Note:
            Sub-classes, most probably, won't need to override that method.

        Args:
            test_list (:obj:`list`): the list of ``CampaignTest`` tests to run.
            extra_arguments (:obj:`list`, optional): a list of extra arguments
                for tests execution.

        Returns:
            list of int: an ordered list of integer codes indicating either
                success (0), aborting (-1) or failure (1).
        """
        return_codes = list()
        for test in test_list:
            if self.dry_run:
                self.log.info("Would have run %s (dry-run).", test.name)
                return_codes.append(RETURN_CODES[SKIPPED])
                continue

            if not test.ts_details.runable:
                return_code = RETURN_CODES[SKIPPED]
                return_codes.append(return_code)
            elif not self.run_in_process:
                self.log.info("Executing %s (out-of-process):", test.name)
                return_code = self._run_test_out_of_process(test, extra_arguments)
                self.rerun_test = self.options.rerun_test
                if self.runner_type is QAC:
                    if return_code not in [0, -1] and self.rerun_test:
                        return_code = self._run_test_out_of_process(test, extra_arguments)
                self.rerun_abort_test = self.options.rerun_abort_test
                if return_code in [-1, 2] and self.rerun_abort_test:
                    self.log.info("Re-running the ABORTED test: {}".format(test.name))
                    return_code = self._run_test_out_of_process(test, extra_arguments)
                return_codes.append(return_code)
            else:
                self.log.info("Executing %s (in-process):", test.name)
                return_code = self._run_test_in_process(test, extra_arguments)
                return_codes.append(return_code)

            if sys.version_info[0] == 3:
                return_code_str = list(RETURN_CODES.keys())[list(RETURN_CODES.values()).index(return_code)]
            elif sys.version_info[0] == 2:
                return_code_str = RETURN_CODES.keys()[RETURN_CODES.values().index(return_code)]
            else:
                raise BaseTestRunnerException("Unsupported Python version")
            self.log.info("%s execution finished with return code %s", test.name, return_code_str)

        return return_codes

    @staticmethod
    def _step_outcome_from_metrics(result_metrics):
        if not result_metrics:
            return None
        outcomes_list = [metric.split(',')[0] for metric in result_metrics]
        steps_list = [outcome[0] for outcome in outcomes_list if outcome]
        return ''.join(steps_list)

    @staticmethod
    def _log_result_matrix(test_list, logger):
        """
        Writes a tests x results matrix to the log.

        Args:
            test_list (:obj:`list`): the list of ``CampaignTest`` tests to
                process.
            logger (:obj:`logging.Logger`): the logger object to log to.
        """
        logger.info("--------------------")
        logger.info("Test Results Summary")
        logger.info("--------------------")
        logger.info("%-40s %-8s %-40s %-8s",
                    "Test name",
                    "Result",
                    "Description",
                    "Run Time(In Seconds)")

        for test in test_list:
            test_name = test.name
            if len(test_name) > 37:
                test_name = test_name[:37] + '...'
            test_description = test.ts_details.description
            if test_description is None:
                test_description = 'This test has no description'
            else:
                test_description = str(test_description)
            if len(test_description) > 60:
                test_description = test_description[:57] + '...'
            if test.is_blocked:
                test_outcome = BLOCKED
            else:
                test_outcome = test.outcome
            test_run_time = test.run_time
            logger.info("%-40s %-8s %-40s %-8s",
                        test_name,
                        test_outcome,
                        test_description,
                        test_run_time,
                        )

        logger.info("--------------------")

    def _generate_junit_file(self, test_list, logger, result_folder,
                             generate_individual_results=False):
        """
        Produces a JUnit XML file from the test results.

        Args:
            test_list (:obj:`list`): the list of ``CampaignTest`` tests to
                process.
            logger (:obj:`logging.Logger`): the logger object to log to.
            result_folder (:obj:`str`): the full path to the output folder
                location.
        """
        logger.info("Generating JUnit report file...")
        test_cases = list()
        time_format = '%Y-%m-%d %H:%M:%S'
        for test_index, test in enumerate(test_list):
            test_name = test_class_name = test.name
            test_outcome = test.outcome
            test_outcome_metrics = None
            test_duration = test.run_time
            test_log = None
            if self.project_type:
                test_outcome_metrics = test.log_messages
                test_log = test.log_file
            if test.record:
                start_time = test.record.get_start_time()
                if start_time:
                    start_time = datetime.strptime(start_time, time_format)
                stop_time = test.record.get_stop_time()
                if stop_time:
                    stop_time = datetime.strptime(stop_time, time_format)

                test_outcome_metrics = test.record.get_messages()
                test_log = test.record.get_logfile()
                if start_time and stop_time:
                    test_duration = (stop_time - start_time).seconds

            if test_outcome_metrics is None:
                test_outcome_metrics = 'Test Metrics are not generated;'

            test_case = TestCase(
                test_name,
                elapsed_sec=test_duration,
                status=test_outcome,
                log=test_log,
            )

            if test_outcome == PASS:
                temp_str = 'Passed:\n'
                for entry in test_outcome_metrics:
                    temp_str += '%s\n' % entry
                test_case.stdout = temp_str
            elif test_outcome == FAIL:
                temp_str = 'Failed:\n'
                for entry in test_outcome_metrics:
                    temp_str += '%s\n' % entry
                test_case.add_failure_info(
                    message="FAILED", output=temp_str
                )
            elif test_outcome == ABORT:
                temp_str = 'Aborted:\n'
                if (test_outcome_metrics is not None):
                    for entry in test_outcome_metrics:
                        temp_str += '%s\n' % entry
                if (test.log_messages is not None):
                    temp_str += test.log_messages
                test_case.add_error_info(
                    message="ABORTED", output=temp_str
                )
            elif test_outcome == SKIPPED:
                temp_str = 'Skipped:\n'
                if (test.log_messages is not None):
                    temp_str += test.log_messages
                test_case.add_skipped_info(
                    message="SKIPPED", output=temp_str
                )

            test_cases.append(test_case)
            if not generate_individual_results:
                continue

            test_suite = TestSuite(
                '%s suite' % test_name,
                test_cases=[test_case]
            )
            if sys.version_info[0] == 3:
                junit_buffer = StringIO()
            elif sys.version_info[0] == 2:
                junit_buffer = StringIO.StringIO()
            TestSuite.to_file(junit_buffer, [test_suite], prettyprint=True)
            # Store individual JUnit structure for any later use:
            test.junit_result = junit_buffer.getvalue()
            junit_buffer.close()

        if self.project_type:
            if self.options.testsetname:
                campaign = ''.join(['TestSet_', self.options.testsetname])
            else:
                campaign = ''.join(['TestSet_', self.options.testsetfoldername])
        else:
            campaign = 'Campaign_' + '_'.join(self.options.campaign.split(','))
        test_suite = TestSuite(
            campaign,
            test_cases=test_cases
        )

        timestamp = time.strftime('%Y%m%d%H%M%S')
        junit_filename = 'TestResultSummary-%s.xml' % timestamp
        junit_filename = os.path.join(result_folder, junit_filename)

        with open(junit_filename, 'w', encoding='utf-8') as junit_file:
            TestSuite.to_file(junit_file, [test_suite], prettyprint=True)

    def report(self, test_list):
        """
        Base runner ``report()`` implementation: calls every registered reporter
        function for an already executed list of tests.

        Note:
            Sub-classes, most probably, won't need to override that method.
            Instead, they should register their own reporter function during
            ``setup()``. The `BaseTestRunner` `reporters` attribute is a list of
            reporters that can be extended. It is expecting tuples of callable
            and dictionary, containing extra arguments to unpack as callable
            arguments at report time. First positional argument of the callable
            should always be the list of tests to process. Below is an example
            of reporter registration, declaring one extra `logger` argument::

                def setup(self):

                    [...]

                    self.reporters += [(
                        self._log_result_matrix, {
                            'logger': self.log,
                        }
                    )]

                @staticmethod
                def _log_result_matrix(test_list, logger):

                    [...]

        Args:
            test_list (:obj:`list`): the list of ``CampaignTest`` tests to
                process.
        """
        if self.reporters:
            for reporter_func, reporter_args in self.reporters:
                reporter_func(test_list, **reporter_args)
        else:
            self.log.info("No reporter registered, not reporting anything...")

    def _read_config_file(self):
        """
        Read test runner input configuration from a simple yml file.
        """
        if self.options.config:
            with open(self.options.config) as config:
                config_file = yaml.safe_load(config)

                if not self.options.tests_path:
                    self.options.tests_path = config_file['tests_path']
                if not self.options.result_folder:
                    self.options.result_folder = config_file['result_folder']
                if not self.options.exec_type:
                    self.options.exec_type = config_file.get('exec_type', 'py')
                if not self.options.test_platform:
                    self.options.test_platform = config_file['test_platform']
                if not self.options.target_device:
                    self.options.target_device = config_file['target_device']
                if not self.options.skip_folders:
                    self.options.skip_folders = config_file['skip_folders']
                if not self.options.campaign:
                    self.options.campaign = config_file['campaign']
                if not self.options.tc_keys:
                    self.options.tc_keys = config_file['tc_keys']
                if not self.options.exclude:
                    self.options.exclude = config_file['exclude']
                if not self.options.intersect:
                    self.options.intersect = config_file['intersect']
                if not self.options.username:
                    self.options.username = config_file['username']
                if not self.options.password:
                    self.options.password = config_file['password']
                if not self.options.QacHost:
                    self.options.QacHost = config_file['QacHost']
                if not self.options.projectname:
                    self.options.projectname = config_file['projectname']
                if not self.options.testsetname:
                    self.options.testsetname = config_file['testsetname']
                if not self.options.testsetfoldername:
                    self.options.testsetfoldername = config_file['testsetfoldername']

    def run(self, options, extra_arguments=None):
        """
        Runs a test campaign according to given options.

        Note:
            This is the main entry-point for any runners. Sub-classes, most
            probably, won't need to override that method.

        Args:
            options (:obj:`argparse.Namespace`): the options holding namespace.
            extra_arguments (:obj:`list`, optional): a list of extra arguments
                for tests execution.
        """
        self.options = options
        self._read_config_file()
        test_paths = self.setup()
        result_list = list()
        skip_test_list = list()
        blocked_test_list = list()
        if not test_paths or not isinstance(test_paths, list) and self.options.rerun_testset:
            return 1

        if not test_paths or not isinstance(test_paths, list):
            # Most probably logger hasn't been setup yet:
            print("Campaign setup failed, returning in error...")
            return 1

        # Skip discover stage for non-UM style HST firmware test framework.
        if not self.options.project_type:
            test_scripts_set = self.discover(test_paths)
            if not test_scripts_set or not isinstance(test_scripts_set, set):
                self.log.error("Tests discovering failed, returning in error...")
                return 2
        else:
            test_scripts_set = None

        if self.runner_type is QAC:
            if self.options.rerun_testset:
                test_list, passed_test_list, blocked_test_list = self.filter(test_scripts_set)
            else:
                test_list, skip_test_list, blocked_test_list = self.filter(test_scripts_set)
        else:
            test_list = self.filter(test_scripts_set)

        if not test_list or not isinstance(test_list, list):
            self.log.error("Test set filtering failed, returning in error...")
            return 3
        if self.runner_type is QAC:
            for test in test_list:
                try:
                    result = self.execute([test], extra_arguments)
                    self.result_qac([test])
                    result_list.append(result)
                except QACConnectionError as error:
                    self.log.error("QAC connection is failed: %s" % str(error))
                    result_list.append(RETURN_CODES[ABORT])
                except Exception as error:
                    self.log.error("Test failed. Error is: %s" % str(error))
                    result_list.append(RETURN_CODES[ABORT])
                flat_result_list = [item for sublist in result_list for item in sublist]
                self.log.info("-" * 20)
                self.log.info("Tests progress update")
                self.log.info("-" * 20)
                self.log.info("No of tests Executed  : {}/{}".format(len(result_list), len(test_list)))
                self.log.info("No of tests Passed  : {}".format(flat_result_list.count(RETURN_CODES[PASS])))
                self.log.info("No of tests Failed  : {}".format(flat_result_list.count(RETURN_CODES[FAIL])))
                self.log.info("No of tests Aborted  : {}".format(flat_result_list.count(RETURN_CODES[ABORT])))
                self.log.info("No of tests Skipped  : {}".format(flat_result_list.count(RETURN_CODES[SKIPPED])))
                self.log.info("No of tests Blocked  : {}".format(flat_result_list.count(RETURN_CODES[BLOCKED])))

            if skip_test_list:
                for test in skip_test_list:
                    try:
                        self.result_qac([test], skip_flag=True)
                        result_list.append(RETURN_CODES[SKIPPED])
                    except QACConnectionError as error:
                        self.log.error("QAC connection is failed: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
                    except Exception as error:
                        self.log.error("Test failed. Error is: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
            if blocked_test_list:
                for test in blocked_test_list:
                    try:
                        self.result_qac([test], block_flag=True)
                        result_list.append(RETURN_CODES[BLOCKED])
                    except QACConnectionError as error:
                        self.log.error("QAC connection is failed: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
                    except Exception as error:
                        self.log.error("Test failed. Error is: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
            if self.options.rerun_testset and passed_test_list:
                for test in passed_test_list:
                    try:
                        self.result_qac([test], pass_flag=True)
                        result_list.append(RETURN_CODES[PASS])
                    except QACConnectionError as error:
                        self.log.error("QAC connection is failed: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
                    except Exception as error:
                        self.log.error("Test failed. Error is: %s" % str(error))
                        result_list.append(RETURN_CODES[ABORT])
        else:
            result_list = self.execute(test_list, extra_arguments)
        if not result_list or not isinstance(result_list, list):
            self.log.error("Tests execution failed, returning in error...")
            return 4

        if self.options.rerun_testset and passed_test_list:
            test_list += passed_test_list
        if skip_test_list:
            test_list += skip_test_list
        if blocked_test_list:
            test_list += blocked_test_list
        self.report(test_list)

        if RETURN_CODES[ABORT] not in result_list and RETURN_CODES[FAIL] not in result_list:
            self.log.info("All test results set to either SKIPPED(dry_run) or PASS or BLOCKED")
            return 0
        else:
            self.log.error("Found failed/aborted tests in the test campaign results")
            return 1


class NonTmsTestRunner(BaseTestRunner):
    """
    The runner class for file-based, command line driven test campaign
    execution.
    """

    def __init__(self):
        """
        Construct an empty :obj:`NonTmsTestRunner` campaign runner instance.
        """
        super(NonTmsTestRunner, self).__init__()

        self.things_to_execute = None
        self.things_to_exclude = None
        self.things_to_intersect = None

    def setup(self):
        """
        Runner ``setup()`` implementation: deal with campaign specific options
        (--campaign, --exclude and --intersect).

        Returns:
            list of str: the list of paths to be analysed.
        """
        self.things_to_execute = self.options.campaign.split(',')

        if self.options.exclude is not None:
            self.things_to_exclude = self.options.exclude.split(',')
        if self.options.intersect is not None:
            self.things_to_intersect = self.options.intersect.split(',')

        # Base argument handling and logging are implement in base class:
        test_paths = super(NonTmsTestRunner, self).setup()

        self.log.info("Tests to be executed '%s'.", self.things_to_execute)
        self.log.info("Tests to be excluded '%s'.", self.things_to_exclude)
        self.log.info("Tests to be intersected '%s'.", self.things_to_intersect)

        return test_paths

    def _list_tests(self, user_inputs, sorted_tests):
        """
        Builds a list of tests matching user inputs (campaigns, modules or test
        names).

        Args:
            user_inputs (:obj:`list` of :obj:`str`): the list of user inputs,
                being either campaigns, modules or test names.
            sorted_tests (:obj:`tuple` of three :obj:`dict`): a triplet of tests
                dictionaries sorted, respectively, by name, module and campaign.

        Returns:
            set of :obj:`CampaignTest`: the set of tests.
        """
        tests_by_name, tests_by_module, tests_by_campaign = sorted_tests

        test_set = set()
        for user_input in user_inputs:
            if not user_input:
                continue
            if user_input in tests_by_campaign:
                test_set.update(tests_by_campaign[user_input])
                continue
            if user_input in tests_by_module:
                test_set.add(tests_by_module[user_input])
                continue
            if user_input in tests_by_name:
                test_set.add(tests_by_name[user_input])
                continue

        return test_set

    def _sort_tests(self, test_set):
        """
        Extract tests by names, modules and campaigns.

        Args:
            test_set (:obj:`set` of :obj:`CampaignTest`): the set of tests to
                sort.

        Returns:
            tuple of three :obj:`dict`: a triplet of tests dictionaries sorted,
                respectively, by name, module and campaign.
        """
        tests_by_campaign = dict()
        tests_by_module = dict()
        tests_by_name = dict()

        for test in test_set:
            test_name = test.name
            # Reference the test by its name:
            tests_by_name[test_name] = test

            test_module = os.path.basename(test.source_file)
            # Reference the test by its module's name:
            tests_by_module[test_module] = test

            # Reference the test by its pertaining campaign's names:
            for campaign in test.campaigns:
                # Tests for a given campaign are stored in a set that we must
                # initialise first. The set then takes care of deduplication.
                if campaign not in tests_by_campaign:
                    tests_by_campaign[campaign] = set()
                # Reference the test by its campaign's name:
                tests_by_campaign[campaign].add(test)

        return tests_by_name, tests_by_module, tests_by_campaign

    def filter(self, test_scripts_set):
        """
        Runner ``filter()`` implementation: filters a given set of tests based
        on the --campaign, --exclude and --intersect options.

        Args:
            test_scripts_set (:obj:`set` of :obj:`CampaignTest`): the set of  tests to
                filter.

        Returns:
            list of :obj:`CampaignTest`: an alphabetically sorted list of tests.
        """
        test_objects_to_execute = []
        self.tests_pattern = self.options.tests_pattern
        if self.tests_pattern is not None:
            print("\nMatched Test Scripts with specified tests_pattern: '{}':".format(self.tests_pattern))
            for test in test_scripts_set:
                match = re.search(self.tests_pattern, test.name)
                if match is not None:
                    print("{}".format(test.name))
                    test_object = CampaignTest(name=test.name, ts_details=test,
                                               results_folder_flag=self.results_folder_flag)
                    test_objects_to_execute.append(test_object)
        else:
            sorted_tests = self._sort_tests(test_scripts_set)
            # First, build a set of --campaign queried tests:
            if 'all' not in self.things_to_execute:
                tests_to_execute = self._list_tests(self.things_to_execute,
                                                    sorted_tests)
            else:
                tests_to_execute = test_scripts_set

            # Intersect it with the --intersect set of tests:
            tests_to_intersect = None
            if self.things_to_intersect:
                tests_to_intersect = self._list_tests(self.things_to_intersect,
                                                      sorted_tests)
            if tests_to_intersect is not None:
                tests_to_execute = tests_to_execute.intersection(tests_to_intersect)

            # Remove the --exclude set of tests form it:
            tests_to_exclude = None
            if self.things_to_exclude:
                tests_to_exclude = self._list_tests(self.things_to_exclude,
                                                    sorted_tests)
            if tests_to_exclude is not None:
                tests_to_execute = tests_to_execute.difference(tests_to_exclude)

            test_inputs = {}
            if self.options.tc_keys:
                tc_key_str = "tc_key"
                self.log.info("List of tc_keys: {}".format(self.options.tc_keys))
                keys_list = self.options.tc_keys.split(',')
                for key in keys_list:
                    for test in tests_to_execute:
                        test_object = CampaignTest(name=test.name, ts_details=test,
                                                   results_folder_flag=self.results_folder_flag,
                                                   test_inputs={tc_key_str : key})
                        test_objects_to_execute.append(test_object)
            else:
                for test in tests_to_execute:
                    test_object = CampaignTest(name=test.name, ts_details=test,
                                               results_folder_flag=self.results_folder_flag,
                                               test_inputs=test_inputs)
                    test_objects_to_execute.append(test_object)

        # Finally, sort the test objects to execute by name:
        return super(NonTmsTestRunner, self).filter(test_objects_to_execute)


class MissingTestReoport:
    def __init__(self, result):
        self.result = result

    def get_result(self):
        return self.result

    def get_start_time(self):
        return 0

    def get_stop_time(self):
        return 0

    def get_logfile(self):
        return "This test is %s " % TEST_STATUS_MAPPING[self.result]

    def get_messages(self):
        return "This test is %s " % TEST_STATUS_MAPPING[self.result]


class MissingTestObject:
    def __init__(self, test_id):
        self.test_id = test_id
        self.__name__ = "MissingTest:{}".format(test_id)


def create_missing_test(test_id, result_status=SKIPPED):
    test_class = MissingTestObject(test_id)
    newTest = TestScriptDetails(name="_".join([test_class.__name__, test_id]),
                                source_file="missing",
                                constructor=test_class,
                                runable=False,
                                description="A missing test for {} test".format(test_id))
    newTest.file = 'missing'
    test_object = CampaignTest(name="_".join([test_class.__name__, test_id]), ts_details=newTest)
    test_object.record = MissingTestReoport(result_status)
    return test_object


class QacTestRunnerException(Exception):
    "Exception class for QacTestRunner"
    pass


class QacTestRunner(BaseTestRunner):
    """
    The runner class for QAC based, command line driven, test campaign
    execution.
    """

    def __init__(self, connection=QACConnection):
        """
        QacTestRunner runner `__init__()` implementation: The setup function extends the
        BaseTestRunner __init__ function for QAC.

        Note:
            connection is the QAC connection to be used, this may be a mock.

        Args:
            connection, factory function that when called should provide a object to interact
            with QAC
        """
        self.connection = connection
        self.conn = None
        self.project = None
        super(QacTestRunner, self).__init__(runner_type=QAC)

    def update_jenkins_url_field(self, testset_id):
        """
        Update Test Library custom field: "Jenkins_url" value with current running Jenkins job URL in specified TestSet.

        Args:
            testset_id: Specify QAC Test Set Id

        Returns:
            None.
        """
        tests_list = self.project.testsets.get_tests_list(testset_id)
        for test_count in range(len(tests_list['results'])):
            qac_test_case_id = tests_list['results'][test_count]['test_id']
            test_data = self.project.tests.get_test(qac_test_case_id)

            all_customs = test_data["custom_fields"]
            for field in all_customs:
                if field['name'] == self.jenkins_url_custom_field:
                    custom_build_id = field['id']
                    cus_jenkins_url = {
                        "CustomFields": [
                            {
                                "Id": custom_build_id,
                                "Name": self.jenkins_url_custom_field,
                                "Value": str(self.current_jenkins_job_number_url)
                            }
                        ]
                    }
                    self.project.tests.update_test(qac_test_case_id, cus_jenkins_url)

    def setup(self):
        """
        QacTestRunner runner `setup()` implementation: The setup function extends the
        BaseTestRunner setup function for QAC.

        Note:
            setup gets the QAC credentials and connects to QAC, the project and testset are
            retrieved before testset work is done.

        Returns:
            :obj:`list` of :obj:`str`: the list of paths to be analysed."""

        if self.options.username is None:
            try:
                username = os.environ["QAC_USER"]
            except KeyError:
                if sys.version_info[0] == 3:
                    username = eval(input("Please enter your cirrus email address: "))
                elif sys.version_info[0] == 2:
                    username = input("Please enter your cirrus email address: ")
        else:
            username = self.options.username

        if not self.options.password:
            try:
                password = os.environ["QAC_PASS"]
            except KeyError:
                password = getpass.getpass("Please enter your password: ")
        else:
            password = self.options.password
        self.conn = self.connection(self.options.QacHost, username, password)
        self.conn.allow_insecure()

        self.jenkins_url_custom_field = "Jenkins_url"
        self.current_jenkins_job_number_url = os.environ.get("BUILD_URL")

        # Retrieve the server version information
        versions = self.conn.versions

        apiver = versions["rest"]

        if parse_version(apiver) < parse_version("11.7"):
            print("Sorry, the API version of the server is too low.")
            print("In order to support test runs, we need QAC 11.7 and we have %s" % apiver)
            raise SystemExit

        try:
            self.project = self.conn.projects[self.options.projectname]
        except KeyError:
            raise QacTestRunnerException("Project '%s' not found in QAC server %s" % (self.options.projectname,
                                                                                      self.options.QacHost))

        release_name = None
        release_id = None
        try:
            release_name = self.options.releasename
        except:
            release_name = None
        if release_name is not None:
            qac_releases = self.project.get_release_records(0, 25)
            total_qac_releases = qac_releases['metadata']['result_set']['total']
            print("Total Release records in Project: {}".format(total_qac_releases))
            releases_offset = 0
            releases_limit = 1000  # If less 1k its taking time to iterate & find.
            while releases_offset < total_qac_releases:
                qac_releases = self.project.get_release_records(releases_offset, releases_limit)
                for release_record in range(len(qac_releases['results'])):
                    qac_release_name = qac_releases['results'][release_record]['title']
                    qac_release_id = qac_releases['results'][release_record]['id']
                    if qac_release_name == release_name:
                        release_id = qac_release_id
                        break
                releases_offset += releases_limit

            if release_id is not None:
                print("Release name read from QAC for this project:%s ", str(release_name))
            else:
                release_id = self.project.releases.add_release(self.options.releasename).id
                print("New release created in QAC:%s ", str(release_name))

        # Get the configuration name
        if self.options.test_config:
            self.configuration_name = self.options.test_config
        elif self.options.test_platform and self.options.target_device:
            self.configuration_name = "_".join([self.options.test_platform, self.options.target_device])
        elif self.options.target_device:
            self.configuration_name = self.options.target_device
        else:
            self.configuration_name = None

        self.rerun_testset = self.options.rerun_testset
        self.testrun_id = self.options.testrun_id
        if self.rerun_testset:
            self.passed_test_objects = []
            if self.testrun_id:
                self.failed_tests_list, run_id = self.get_failed_tests(self.testrun_id)
            else:
                self.failed_tests_list, run_id = self.get_last_run_failed_tests()

        if self.rerun_testset and not self.failed_tests_list:
            print("All tests are passed in the given run: %s, so exiting." % run_id)
            return self.failed_tests_list

        if self.options.testsetname:
            self.testruns = []
            test_sets = []
            try:
                for test_set in self.options.testsetname.split(';'):
                    test_sets.append(self.project.testsets[test_set])
            except KeyError:
                raise QacTestRunnerException("Test set '%s' not found in QAC server %s" % (self.options.testsetname,
                                                                                           self.options.QacHost))
            self.testset_id = None
            for test_set_obj in test_sets:
                self.testset_id = test_set_obj.id
                if self.testset_id is not None:
                    self.update_jenkins_url_field(self.testset_id)

            for test_set_obj in test_sets:
                if test_set_obj.is_sequential:
                    self.is_sequential = test_set_obj.is_sequential

            for test_set in test_sets:
                self.get_target_config_dict(test_set)
                try:
                    testrun_obj = test_set.start_run(release_id=release_id, configuration=self.testconfiguration)
                    self.testruns.append(testrun_obj)
                except QACConnectionError:
                    (exception_type, value, traceback_msg) = sys.exc_info()
                    known_message = "Unable to create a test run, since no active test is specified."
                    if known_message in str(value):
                        raise BaseTestRunnerException("Specified TestSet should have at least one active test: {}"
                                                      .format(test_set.title))
        else:
            try:
                from qacomplete import folder
            except ImportError:
                raise Exception("No folder support in python-QAComplete please upgrade python-QAComplete")

            self.testruns = []
            searchlen = len(self.options.testsetfoldername)
            for testset in self.project.testsets:
                if testset.folder and len(testset.folder) >= searchlen and \
                        testset.folder[:searchlen] == self.options.testsetfoldername:
                    self.get_target_config_dict(testset)
                    self.testruns.append(testset.start_run(release_id=release_id,
                                                           configuration=self.testconfiguration))

        # these must be in this order so that junit results are available when result_qac is called
        self.individual_results = True
        result = super(QacTestRunner, self).setup()

        # Base argument handling and logging are implement in base class:
        return result

    def get_testset_configurations(self, testset_id):
        """
        Returns configurations dict of a testset.
        @param testset_id: testset id
        @return: dict of configurations
        """
        configurations = {}
        json_data = self.project.linkeditems("TestSets", testset_id)
        json_data = json_data['results']
        for item in json_data:
            if item['linked_entity_code'] == 'TestConfigurations':
                configurations[item['title']] = item['linked_entity_id']
        return configurations

    def get_target_config_dict(self, test_set):
        """
        Returns target config dict for the given input option --test_config
        @param test_set: TestSet object
        @return: dict {<configuration_name>:<configuration_id>}
        """
        test_set_configurations = self.get_testset_configurations(test_set.id)
        if self.configuration_name in test_set_configurations.keys():
            self.testconfiguration = {self.configuration_name: test_set_configurations[self.configuration_name]}
        else:
            self.testconfiguration = None
            print("Test configuration '%s' not exist, but executing test without setting test "
                  "configurations" % self.configuration_name)
        return self.testconfiguration

    def get_last_run_failed_tests(self):
        """
        Gets the Failed tests list in the last run in combination with Testset and Test Configuration

        Returns: list : Failed tests list
        """
        target_test_runs = []
        target_test_runs_json = []
        _test_runs = self.project.testruns

        for testrun in _test_runs:
            if testrun.title not in [self.options.testsetname] or testrun.status == "Passed" or \
                    self.options.test_config != testrun.configuration_name:
                continue
            else:
                target_test_runs.append(testrun)
                target_test_runs_json.append(testrun.content)
        # Gets latest run
        if len(target_test_runs_json) > 0:
            target_test_runs_json = sorted(target_test_runs_json, key=lambda k: k['date_started'], reverse=True)
        else:
            raise QacTestRunnerException("No failed runs available with given Testset/Configuration combination.")

        latest_run_id = target_test_runs_json[0]['id']
        print("\nLast run id of test set %s with the configuration %s: %s" % (self.options.testsetname,
                                                                              self.options.test_config,
                                                                              latest_run_id))
        failed_tests_list, run_id = self.get_failed_tests(latest_run_id)
        return failed_tests_list, run_id

    def get_failed_tests(self, run_id):
        """
        Gets the Failed tests list of the specified run.

        @param run_id: integer - Run id of the testrun
        @return: list - List of failed tests
        """
        failed_tests = []
        test_run = self.project.testrun(run_id)
        if self.testrun_id:
            # Overriding the testsetname option by getting the testset name from the run ID.
            self.options.testsetname = test_run.title
        for _testRequirement in test_run.items:
            if _testRequirement.status != "Passed":
                failed_tests.append(_testRequirement.title)
        print("Failed tests in the run Id: %s are: %s\n" % (run_id, failed_tests))
        return failed_tests, run_id

    def filter(self, test_scripts_set):
        """
        QacTestRunner runner `filter()` implementation: filters for test in the QAC testset.

        Note:
            Sub-classes should always chain-up to that base implementation in
            order to respect base filtering behaviour, if they need to override
            that method.

        Args:
            test_scripts_set (:obj:`set`): the set of ``CampaignTest`` tests to filter.
        Returns:
            :obj:`list`: an alphabetically sorted list of ``CampaignTest`` test
                objects.
        """

        test_objects_to_execute = []
        # This is to collect the tests that needs to be skipped without executing those. This is required when we run
        #  the tests with --campaign or --exclude options we may have to skip some tests directly without running.
        test_objects_to_skip = []

        # List for capturing all blocked tests.
        test_objects_to_block = []

        campaign_option = self.options.campaign
        exclude_option = self.options.exclude
        if campaign_option and campaign_option != 'all':
            target_qac_tests = campaign_option.split(",")
        if exclude_option:
            exclude_qac_tests = exclude_option.split(",")
        if not self.project_type:
            ts_details_by_class = {}
            for testobject in test_scripts_set:
                try:
                    ts_details_by_class[testobject.name] = testobject
                except AttributeError:
                    self.log.warning("Test script does not have a test class name")
        for testrun in self.testruns:
            self.log.info("QAC Test Run ID: %s" % testrun.id)
            target_device = None
            if testrun.device and testrun.target:
                target_device = "_".join([self.options.projectname.lower(), testrun.device, testrun.target])
            elif testrun.target:
                target_device = "_".join([self.options.projectname.lower(), testrun.target])
            test_platform = testrun.os_platform
            try:
                test_run_items = testrun.items
            except TypeError:
                test_run_items = None
            if test_run_items is not None:
                for testRequirement in testrun.items:
                    test = testRequirement.test
                    customs = test.customfields
                    all_customs = test.get_all_custom_fields
                    self.log.info("CustomFields in this tests are: %s", customs)
                    build_number = self.options.buildId
                    for field in all_customs:
                        if field['name'] == "build_id":
                            custom_build_id = field['id']
                            if build_number is not None:
                                build_id = {"CustomFields": [
                                    {"Id": custom_build_id, "Name": "build_id", "Value": str(build_number)}]}
                                self.log.info("Jenkins build with current testruns is: %s", build_number)
                                self.project.tests.update_test(test.id, build_id)
                            break
                    # Read the custom field 'is_blocked'
                    is_blocked_str = "is_blocked"
                    if customs.get(is_blocked_str) == "Yes":
                        self.is_blocked = customs.get(is_blocked_str)
                    else:
                        self.is_blocked = False
                    # Test script class name
                    ts_class = customs["Cirrus_Test_Id"]
                    status = test.status
                    qac_test_name = ts_class
                    try:
                        if status not in INVALID_TESTCASE_STATUS:
                            if not self.project_type:
                                ts_details = ts_details_by_class[ts_class]
                            else:
                                # Construct test objects with just the QAC Cirrus_Test_id.
                                # Since we skipped discover and filter for HST type of tests,
                                # we do not have any test script details stored
                                ts_details = TestScriptDetails(name=ts_class)
                            test_inputs = test.download_file('input_parameters.yml')

                            # If TC_key is defined and has a valid value in Test Case custom fields, add it to test_inputs
                            tc_key_str = "tc_key"
                            if customs.get(tc_key_str):
                                test_inputs[tc_key_str] = customs.get(tc_key_str)

                            if test_inputs:
                                qac_test_name = "-".join([ts_class, "QacId" + str(testRequirement.id)])

                            test_object = CampaignTest(name=qac_test_name,
                                                       ts_details=ts_details,
                                                       test_id=testRequirement.id,
                                                       test_inputs=test_inputs,
                                                       target_device=target_device,
                                                       test_platform=test_platform,
                                                       results_folder_flag=self.results_folder_flag,
                                                       is_blocked=self.is_blocked)
                            if ts_details.error_msg != None:
                                self.log.warning("QAC Test case %s is discovered but "
                                                 "skipped due to below error:\n %s", ts_class, ts_details.error_msg)
                                test_object.record = MissingTestReoport('SKIPPED')
                        else:
                            self.log.warning("QAC Test case: %s is not valid status: %s."
                                             " Creating a missing test case status defaulting to 'Skipped' "
                                             "in test results summary" % (qac_test_name, status))
                            test_object = create_missing_test(ts_class)
                    except KeyError:
                        self.log.warning("QAC Test case %s (Cirrus_Test_Id) not found in the files "
                                         "discovered in the filesystem. Creating a missing test case status "
                                         "defaulting to 'Skipped' in test results summary", ts_class)
                        test_object = create_missing_test(ts_class)
                    test_object.QacTestObject = testRequirement
                    if self.rerun_testset:
                        if test.title in self.failed_tests_list:
                            test_objects_to_execute.append(test_object)
                        else:
                            if test_object.is_blocked:
                                test_objects_to_block.append(test_object)
                            else:
                                self.passed_test_objects.append(test_object)
                    else:
                        # Collecting test objects to execute based on the value of --campaign
                        # Ignoring default campaign value 'all' as this is not a CirrusTestId. This 'all' is
                        # applicable only for Non QAC runner option.
                        if campaign_option and campaign_option != 'all':
                            if ts_class in target_qac_tests:
                                if test_object.is_blocked:
                                    test_objects_to_block.append(test_object)
                                else:
                                    test_objects_to_execute.append(test_object)
                            else:
                                test_objects_to_skip.append(test_object)
                        # Collecting test objects to exclude based on the value of --exclude
                        elif exclude_option:
                            if ts_class not in exclude_qac_tests:
                                if test_object.is_blocked:
                                    test_objects_to_block.append(test_object)
                                else:
                                    test_objects_to_execute.append(test_object)
                            else:
                                test_objects_to_skip.append(test_object)
                        else:
                            if test_object.is_blocked:
                                test_objects_to_block.append(test_object)
                            else:
                                test_objects_to_execute.append(test_object)

        self.log.info(str(test_objects_to_execute))

        # Skip base runner class' filter stage.
        if self.project_type:
            test_names = ",".join(["%s" % test_name for test_name in test_objects_to_execute])
            self.log.info("List of QAC tests to be executed: %s", test_names)
        elif len(test_objects_to_execute) is not 0:
            # Finally, sort the test objects to execute by name:
            test_objects_to_execute = super(QacTestRunner, self).filter(test_objects_to_execute)
        if self.rerun_testset:
            return test_objects_to_execute, self.passed_test_objects, test_objects_to_block
        else:
            return test_objects_to_execute, test_objects_to_skip, test_objects_to_block

    def result_qac(self, testobjects, pass_flag=False, skip_flag=False, block_flag=False):
        """
        QacTestRunner runner ``result_qac()`` implementation: A reporter function
        to be called by the base classes report function.

        Note:
            this functions is only called if it has been added to the reports list eg.

                def setup(self):

                    [...]

                    self.reporters += [(
                        self.result_qac, {
                        }
                    )]

        Args:
            testobjects (:obj:`list`): the list of ``CampaignTest`` tests to process.
            pass_flag - boolean: This flag is for to update the teststatus to be passed forcefully. There some cases
            while re-run the tests we need to update the teststatus as Passed for previously passed tests without
            running them.
            skip_flag - boolean: This flag is for to update the teststatus to be Skipped forcefully. There are some
            cases while running the tests with --campaign/--exclude we need to update the teststatus as Skipped for
            non-executed tests.
        """
        if not self.dry_run:
            self.log.info("Test-runner preparing to upload to QAC")
            for testobject in testobjects:
                QacTestObject = testobject.QacTestObject
                testruntime = testobject.run_time
                # This condition just checks for pass_flag and skip_flag, if both are false which means the test has
                # been executed and that status will be updated to qac later. If one of this is True, that means the
                # test is not executed but needs to update the teststatus either Passed or Skipped based on which
                # flag is set.
                if not pass_flag and not skip_flag and not block_flag:
                    try:
                        teststatus = testobject.record.get_result()
                    except AttributeError:
                        if self.um_style and testobject.return_code != RETURN_CODES[SKIPPED]:
                            self.log.warning("Test aborted in __init__, no result produced for the test case %s. "
                                             "Override test status to ABORT", testobject.name)
                        teststatus = list(RETURN_CODES.keys())[
                            list(RETURN_CODES.values()).index(testobject.return_code)]
                        if teststatus == RETURN_CODES[SKIPPED]:
                            self.log.warning("Test skipped for the test case %s ", testobject.name)
                elif pass_flag:
                    teststatus = PASS
                elif skip_flag:
                    teststatus = SKIPPED
                elif block_flag:
                    teststatus = BLOCKED
                for iteration in range(2):
                    retry_flag = False
                    try:
                        if teststatus in VALID_TEST_RESULT and testruntime is not None:
                            qac_test_status = TEST_STATUS_MAPPING[teststatus]
                            QacTestObject.patch(status=qac_test_status, runtime=testruntime)
                            if sys.version_info[0] == 3:
                                file_like_results = StringIO()
                                file_like_results.write(str(testobject.junit_result))
                            elif sys.version_info[0] == 2:
                                file_like_results = StringIO.StringIO()
                                file_like_results.write(testobject.junit_result)
                            file_like_results.seek(0)
                            # QacTestObject.upload_log(file_like_results)
                            file_like_results.close()
                            self.log.info("Test-runner uploaded {} results to QAC".format(str(testobject.name)))
                        else:
                            qac_test_status = TEST_STATUS_MAPPING[teststatus]
                            QacTestObject.patch(status=qac_test_status, runtime=0)
                        break
                    except Exception as error:
                        self.log.error("Unable to upload test results for test %s. error:%s",
                                       testobject.name, get_stack_trace())
                        retry_flag = True
                    if retry_flag:
                        traceback = get_stack_trace()
                        known_message = "Connection aborted"
                        if known_message in str(traceback):
                            self.log.info("Re-trying ...")
                            continue
                        else:
                            raise QACConnectionError("Unhandled exception: {}".format(str(traceback)))

            self.log.info("Test-runner uploaded to QAC. Test run URLs below:")
            for test_run in self.testruns:
                self.log.info(
                    "%s: \n%sCommon/?ProjId=%s&Tab=TestManagement&Form=RunHistory&Action=testrunner&id=%s",
                    test_run.title, self.conn._url("", ""), self.project.id, test_run.id)
        else:
            self.log.info("It was a dry-run, no results to upload to QAC")


def main():
    """
    Main entry point for the runner.
    """
    parser = BaseTestRunner.get_argument_parser()
    options, extra = parser.parse_known_args()
    if options.non_tms:
        runner = NonTmsTestRunner()
    else:
        runner = QacTestRunner()

    options, extra = parser.parse_known_args()
    return_code = runner.run(options, extra)
    sys.exit(return_code)


if __name__ == '__main__':
    main()
