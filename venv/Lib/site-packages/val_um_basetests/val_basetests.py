"""
Base class that implements common Validation test functions and patterns. All Validation base tests should inherit from
this base test.
"""
import csv
import inspect
import os
import sys
import subprocess
from copy import deepcopy
from importlib import import_module
from pathlib import Path
from typing import Any, List, TYPE_CHECKING

import cl_test_station.utilities.reusables as reusables
import pandas
import yaml
from cl_test_station.components.component.component import Component
from cl_test_station.test_station import TestStation
from um_framework.um_baseclass import UmBaseTest
from um_framework.um_framework import UmFramework
from unified_modules.test_support.testbaseclass import get_stack_trace
from unified_modules.test_support.testresults import TestResults

if TYPE_CHECKING:
    from val_um_basetests.test_station_class import TEST_STATION_CLASS

try:
    import unified_modules.test_support.testlogger as logging
    import logging
except ImportError:
    pass


class IterationFormatError(Exception):
    pass


def get_active_config_path() -> str:
    """
    Uses the ACTIVE_PROJECT_CONFIG, CONFIGS env variables and grabs the corresponding yaml config path.

    :return: Path to config file
    :rtype: str
    """
    # Get env variable value
    active_config = os.environ['ACTIVE_PROJECT_CONFIG']
    # get name of project config yaml file
    cfg_fname = active_config.lower()
    if '.yml' not in active_config:
        cfg_fname += '.yml'
    # Get absolute path of file from input_data
    ts_cfg_fname = os.path.normpath(os.path.expandvars('$CONFIGS/project_configs/') + cfg_fname)
    if not os.path.exists(ts_cfg_fname):
        raise FileNotFoundError(ts_cfg_fname)
    return ts_cfg_fname


class ValBaseTest(UmBaseTest):
    """Base test class for all Validation projects"""

    def __init__(self, result=None, test_system=None, resources=None, test_platform='win10', local_resources=[]):
        argv = deepcopy(sys.argv)
        super().__init__(result, test_system, resources, test_platform, local_resources)
        sys.argv = argv
        self.log_test_requirements = True
        self.required_args = self.__get_positional_args()
        self.iterations_csv_name = None
        self.continue_iterations_on_error = True
        self.test_station: TEST_STATION_CLASS = None

    def setup(self):
        super().setup()
        try:  # Add Test log handler to test station logger
            self.test_station.log.logger.addHandler(self.log.logger.handlers[0])
        except Exception:
            pass
        # Initilialize iterations
        self.__setup_iterations()

    def __get_positional_args(self) -> List[str]:
        """
        Gets signatures of the test's initiate and complete methods and returns a list of the required positional
        arguments.

        :return: List of positional argument names
        :rtype: List[str]
        """
        args = set()
        kinds = [inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]
        for func in [self.initiate, self.complete]:  # Check both functions
            sig = inspect.signature(func)
            for param in sig.parameters.values():
                if param.kind in kinds and param.default is inspect._empty:  # This is a required positional argument
                    args.add(param.name)
        return list(args)

    def __setup_iterations(self):
        """
        Searches the test's containing directory for a csv or xml file of the same name and loads iteration data. The
        iterations will be looped over in the run_test() function.

        :return: None
        :rtype: None
        """
        fugue_path = r'C:\Program Files\Cirrus Logic\Iteration Designer\Iteration Designer.exe'
        test_file = os.path.normpath(inspect.getfile(self.__class__))
        test_basename = os.path.splitext(test_file)[0]
        csv_path = None
        if self.iterations_csv_name is not None and os.path.isfile(self.iterations_csv_name):
            csv_path = self.iterations_csv_name
        elif os.path.isfile(test_basename + '.csv'):  # Search for csv file
            csv_path = test_basename + '.csv'
        elif os.path.isfile(test_basename + '.xml'):  # Search for fugue xml to generate csv
            # todo: Fugue CLI: give it the source xml and path for the generated csv
            # todo: Use abs path to fugue exe
            self.log.info(f'Iteration Design file {test_basename}.xml found but no corrseponding CSV is found. '
                          f'Building CSV file {test_basename}.csv')
            process = subprocess.Popen([fugue_path, test_basename + '.xml', test_basename + '.csv'], stderr=subprocess.PIPE,
                                       shell=True)
            result, err = process.communicate()
            if err:  # Couldn't generate csv file, raise excpetion
                raise Exception(err.decode('utf-8'))
            csv_path = test_basename + '.csv'

        if csv_path:  # File found or generated, load into dataframe
            dframe = pandas.read_csv(csv_path)
            self.iterations = dframe[dframe['Enabled'] == True]  # Remove disabled iterations
            if self.iterations.empty:  # Check if any iterations are enabled
                raise IterationFormatError(f'No valid iterations found, please input valid csv file: {os.path.test_basename(csv_path)}')
            missing_args = [arg for arg in self.required_args if arg not in self.iterations.columns]
            if missing_args:  # Check for missing required arguments in initiate() and complete()
                raise IterationFormatError(f'Required arguments {", ".join(missing_args)} do not have columns in CSV '
                                           f'"{os.path.test_basename(csv_path)}", either add to csv or define default values')
        else:  # Init empty DataFrame
            self.iterations = pandas.DataFrame(data=[True], columns=['Enabled'])

        # Initialize Dataframe pointers
        self.iteration_index = None
        self.prev_iteration = None

        self.log.info(f'Running iterations from: {csv_path}')

    def initiate(self, **kwargs):
        super().initiate()

    def complete(self, **kwargs):
        super().complete()

    def run_test(self):
        """
        Same logic as UM Test BaseClass, but overrides parameter_list_list and instead uses Pandas Dataframe

        :return: Error, if any, that occurs during test run
        :rtype: Exception
        """
        error = None
        for iteration in self.iterations.itertuples():
            # Convert Pandas tuple into dictionary
            iteration_kwargs = iteration._asdict()
            if 'Enabled' in iteration_kwargs and not iteration_kwargs.pop("Enabled"):  # Skip disabled iterations
                continue
            self.prev_iteration = self.iteration_index
            self.iteration_index = iteration_kwargs.pop('Index')

            # New Test Step if the iteration has any content
            if iteration_kwargs:
                params_repr = []
                for param_name, value in iteration_kwargs.items():
                    params_repr.append('%s = %s' % (str(param_name), str(value)))
                self.result.test_step(f'Iteration for parameters: {", ".join(params_repr)}')

            self.shared_data.initiates_done = False
            try:
                self.initiate(**iteration_kwargs)  # Call initiate with keyword arguments passed from dictionary
                # Run any nested tests, provided initiate completed successfully
                for nested_test in self.nested_tests:
                    # Resources and options are shared
                    nested_test.resources = self.resources
                    nested_test.options = self.options
                    temp_error = nested_test.run_test()
                    # Pass any errors back up the call chain
                    if temp_error is not None:
                        error = temp_error

                # Signal all initiates executed
                if not self.nested_tests == []:
                    self.shared_data.initiates_done = True

                try:
                    self.complete(**iteration_kwargs)  # Call complete with kwargs as well
                except Exception as temp_error:  # Catch any abort and record this in results
                    if self.continue_iterations_on_error:
                        error = temp_error
                        self.result.assert_outcome(html=False)
                        self.result.test_summary(html=False)
                        self.report_abort("%s" % (get_stack_trace(self.full_stack_trace)))
                    else:
                        raise temp_error

            # Catch any abort and record this in results
            except Exception as temp_error:
                if self.continue_iterations_on_error:
                    error = temp_error
                    self.result.assert_outcome(html=False)
                    self.result.test_summary(html=False)
                    self.report_abort("%s" % (get_stack_trace(self.full_stack_trace)))
                else:
                    raise temp_error
            self.iteration_count += 1
        return error

    def variable_needs_config(self, param_name: str) -> bool:
        """
        Checks if test parameter 'param_name' has changed in current iteration.

        :param param_name: name of the test parameter to check
        :type param_name: str
        :return: Returns True if value of test parameter 'param_name' from current iteration differs from previous
                 version, False otherwise
        :rtype: bool
        """
        if param_name not in self.iterations:
            param_list = list(self.iterations.keys())
            param_list.remove('Enabled')
            raise KeyError(f"Invalid parameter name. Parameters for this test: {', '.join(param_list)}")
        if not self.iteration_index or self.prev_iteration is None:  # First iteration has no previous to compare to
            return True
        # Compare values and return
        return self.iterations.loc[self.iteration_index][param_name] != self.iterations.loc[self.prev_iteration][param_name]

    def teardown(self):
        super().teardown()
        if self.log_test_requirements:
            self.write_requirements()
            self.write_test_env_file()

    @staticmethod
    def get_test_station_class(ts_cfg_path: str) -> Any:
        """
        Reads test station config file and imports TestStation class to instantiate. Generates type checking file.

        :param ts_cfg_path: Path to TestStation config YAML file
        :type ts_cfg_path: str
        :return: TestStation class reference
        :rtype: Any
        """
        # Read in template contents
        path = os.path.dirname(inspect.getfile(ValBaseTest))
        with open(os.path.join(path, 'test_station_class_template.py'), 'r') as f:
            template_contents = f.read()
        # Open config path and load test station config into dictionary
        with open(ts_cfg_path, 'r', encoding='utf-8') as f:
            ts_config_dict = yaml.load(f, Loader=yaml.SafeLoader)
        # Parse class type and replace template contents
        class_type: str = ts_config_dict.get('test_station_class_type')
        if class_type:
            module, _, class_name = class_type.rpartition('.')
        else:
            module = TestStation.__module__
            class_name = TestStation.__name__
        template_contents = template_contents.replace('TsClass', class_name).replace('ts_module', module)
        # Write new contents to template file
        with open(os.path.join(path, 'test_station_class.py'), 'w', encoding='utf-8') as f:
            f.write(template_contents)
        # Import modules the return Test Station class reference
        module = import_module(module)
        return getattr(module, class_name)

    def setup_system(self):
        """
        This function follows the standard input_data directory structure used by Validation.
        Requires environment variables:
            * ACTIVE_PROJECT_CONFIG:  Filename of the config yaml file
            * CONFIGS: Path to config directory that has 'project_configs' subdirectory

        :return: None
        :rtype: None
        """
        ts_cfg_path = get_active_config_path()
        ts_cls = self.get_test_station_class(ts_cfg_path)
        # Create empty test station
        self.test_station = ts_cls(config=ts_cfg_path, logger=self.log)
        # Construct all objects
        self.test_station.construct_system()

    def log_transactions(self, filepath, *args):
        """
        Starts logging all register and field transactions into csv file.
        :param filepath: path to csv file
        :param args: specific components to track and log. If none are specified, then all components are tracked
        :return:
        """
        if not filepath.endswith('.csv'):
            raise ValueError("Log file must be csv")
        if args:
            for component in args:
                if isinstance(component, Component):
                    component.enable_logging = True
                    component.log_path = filepath
                else:
                    raise TypeError("Argument must be of type Component, got " + str(component))
        else:
            for key in self.__dict__:
                attr = getattr(self, key)
                if isinstance(attr, Component):
                    attr.enable_logging = True
                    attr.log_path = filepath

        with open(filepath, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Component', 'Address', 'Block', 'Register', 'Field',
                             'Read Value (hex)', 'Read Value (int)', 'Write Value (hex)', 'Write Value (int)'])

    def stop_trans_logging(self, *args):
        """
        Stops logging of transactions for given components
        :param args: components to stop tracking. If none given, all logging is disabled
        :return:
        """
        if args:
            for component in args:
                if isinstance(component, Component):
                    component.enable_logging = False
                    component.log_path = None
                else:
                    raise TypeError("Argument must be of type Component, got " + str(component))
        else:
            for key in self.__dict__:
                attr = getattr(self, key)
                if isinstance(attr, Component):
                    attr.enable_logging = False
                    attr.log_path = None
        return 0

    def write_requirements(self, class_name: str = None, tests_folder: str = 'val_tests'):
        """
        Generates a <test_name>_requirements.txt file that is a snapshot of all of the library versions (including
        source code libraries). For libraries that are pulled in from Git, will record the 10 digit commit ID hash.

        :param tests_folder: name of the folder that contains tests
        :type tests_folder: str
        :param class_name: Name of test class
        :type class_name: str
        :return:
        :rtype:
        """
        # Get all package information minus editable libraries
        proc = subprocess.Popen('pip freeze --exclude-editable', stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                shell=True)
        output, err = proc.communicate()
        pip_freeze_output = output.decode("utf-8").replace('\r', '')

        # If class_name is defined, use that class to get the file path
        test_file = None
        if class_name:
            classes = self.test_station.load_classes_from_path(
                os.path.expandvars('$PROJECT_ROOT') + f'\\{tests_folder}',
                tests_folder)
            try:
                test_file = Path(inspect.getfile(classes[class_name]))
            except KeyError:
                self.test_station.log.error(f'Unable to find test class {class_name}')
        else:
            # Get the path of the current test
            test_file = Path(inspect.getfile(self.__class__))

        # If the test_file is None, this means an exception occurred above and we should not try to do the operation
        if test_file:
            test_path = test_file.parent

            # Create file next to test named <module_name>_requirements.txt
            test_req_txt = str(test_path / f'{test_file.stem}_requirements.txt')
            with open(test_req_txt, 'w') as f:
                f.write(pip_freeze_output)
                # Get the library info, see get_lib_info() docstring for description
                libs, dirty_repos = reusables.get_editable_lib_info()
                if dirty_repos:
                    self.test_station.log.warning('Repos with modified or untracked files detected',
                                                  suppress_file_msg=True)
                    self.test_station.log.warning("Commit any files needed for test and run self.write_requirements"
                                                  "('%s') in the Python Console before submitting the test" %
                                                  self.__class__.__name__, suppress_file_msg=True)
                    for repo, status in dirty_repos.items():
                        self.test_station.log.warning(f'Repo: {repo}', suppress_file_msg=True)
                        # status is a tuple of lists (untracked files, modified files)
                        self.test_station.log.warning(f'\tUntracked Files:', suppress_file_msg=True)
                        for untracked_file in status[0]:
                            self.test_station.log.warning(f'\t\t{untracked_file}', suppress_file_msg=True)
                        self.test_station.log.warning(f'\tModified Files:', suppress_file_msg=True)
                        for modified_file in status[1]:
                            self.test_station.log.warning(f'\t\t{modified_file}', suppress_file_msg=True)
                for lib_name, info in libs.items():
                    # lib_name is the package name of the library
                    # info is a tuple with url and commit ID information
                    line = f'-e git+{info[0]}@{info[1]}#egg={lib_name}\n'
                    f.write(line)

    def write_test_env_file(self):
        """
        Reads station env files in /env/stations/proj_name_val_STATION_NAME and copies it to a test .env file in the
        same directory as the test.

        :return: None
        :rtype: None
        """
        missing = [env_var for env_var in ['PROJECT_ROOT', 'STATION_NAME'] if env_var not in os.environ]
        if missing:
            self.log.warning(f"Missing environment variable{'s' if len(missing) > 1 else ''} {', '.join(missing)}, "
                             f"cannot write test .env file")
            return
        test_module_path = inspect.getfile(self.__class__)
        test_module_dir = os.path.dirname(test_module_path)
        test_module_name = os.path.splitext(os.path.basename(test_module_path))[0]
        station_env_name = f"{test_module_name}_env.env"
        from glob import glob
        env_files = glob(os.path.expandvars("$PROJECT_ROOT/env/station/*_$STATION_NAME.env"))
        if env_files:
            station_env_path = env_files[0]
        else:
            self.log.warning("No station .env files found")
            return
        with open(station_env_path, 'r', encoding='utf-8') as f:
            env_contents = f.read()
        with open(os.path.join(test_module_dir, station_env_name), 'w', encoding='utf-8') as f:
            f.write(env_contents)


if __name__ == "__main__":
    import sys

    TEST = ValBaseTest(TestResults(), UmFramework())
    RETURN_CODE, _ = TEST.run()
    sys.exit(RETURN_CODE)
